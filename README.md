# Replicable neural network


To build upon the MLP above it makes more sense to add more replicability to create neural nets quicker, easier, and more efficient. This code does exactly this as well as some other improvements on the original code. 

1) First it adds a batch normalization feature. This normalizes each layer to have a normal distribution and allows for better training.
2) We add functions that creates the layers, embedding, and tanh normalization so we don't have to worry about creating the layers, individual weights, and biases. Running it through functions allows us to immediately create neural nets without worrying about any specifics.

This is done quite simply through this: 
```
n_embd = 24
n_hidden = 128


model = LayerStack([
      Embedding(vocab_size, n_embd),
      Flatten(2), DenseLayer(n_embd * 2, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),
      Flatten(2), DenseLayer(n_hidden*2, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),
      Flatten(2), DenseLayer(n_hidden*2, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),
      DenseLayer(n_hidden, vocab_size),
    ])
```

Inside to out it does something like this.

1) Creates an embedding layer and a final layer as well as 3 hidden layers. These hidden layers are flattened to allow for better passing into the linear layer.
2) all these layers are then batchnormalized and normalized with tanh
3) We can then run them into a layer stack creating the entire neural network.

Now, we are able to train the model with something along these lines:

```
max_steps = 200000
batch_size = 32

for i in range(max_steps):
  

    ix = torch.randint(0, Xtr.shape[0], (batch_size,))
    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y
    

    logits = model(Xb)
    loss = F.cross_entropy(logits, Yb) 
  

    for p in parameters:
        p.grad = None
    loss.backward()
  
    lr = 0.1 if i < 150000 else 0.01 
    for p in parameters:
        p.data += -lr * p.grad
```


Step by step this creates batches, runs them in the model, finds the loss, runs a backward pass to find gradients, and finally changes gradients to improve loss.



The final loss after training we were able to get: 

```
140000/ 200000: 1.4864
150000/ 200000: 1.8600
160000/ 200000: 2.0416
170000/ 200000: 1.7932
180000/ 200000: 1.8300
190000/ 200000: 1.8756
```
And on sets untrained, we get: 
```
train 1.790572166442871
val 1.790382742881775
```

This is a significant improvement from above and goes to show how using more layers, dimenstionality, and steps all come together to create much better training. The words generated by this model go as follows:

```
faulvering.
retroscopically.
dassto.
whittends.
tacaraeman.
figurates.
fundlindtomy.
proplight.
subduck.
submediator.
quiformations.
blinds.
spruncy.
naooteroscirs.
aurilia.
bargelike.
badzaz.
nonprofublever.
dippense.
rankering.
```

All of these look like real words and some even are real words. This is because our set of words is only 30,000. It could be interesting to use the entire english dictionary to see if training data/words are much better. Something I might do in the near future when I get a new laptop.